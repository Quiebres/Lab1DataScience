# Exploración rápida del dataset usando un resumen
summary(train$Utilities)
# Exploración rápida del dataset usando un resumen
summary(train$LotConfig)
# Exploración rápida del dataset usando un resumen
summary(train$MSSubClass)
# Exploración rápida del dataset usando un resumen
summary(train$Neighborhood)
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
# Exploración rápida del dataset usando un resumen
summary(train$Condition2)
# Exploración rápida del dataset usando un resumen
summary(train$BldgType)
# Exploración rápida del dataset usando un resumen
summary(train$OverallCond)
# Exploración rápida del dataset usando un resumen
summary(train$YearBuilt)
# Exploración rápida del dataset usando un resumen
summary(train$RoofMatl)
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
pairs(trainCuan, lower.panel = NULL)
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
install.packages("corrplot")
install.packages("corrplot")
?cor
?round
# Elaboración de la matriz de correlación
round(cor(trainCuan),3)
# Se visualiza la matriz de correlación de forma gráfica
corrplot(mcorrelacion, method="number", type="upper")
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
install.packages(c("GGally", "Hmisc", "PerformanceAnalytics"))
# Se visualiza la matriz de correlación de forma gráfica
corrplot(mcorrelacion, method="number", type="upper")
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
# Se visualiza la matriz de correlación de forma gráfica
corrplot(mcorrelacion, method="number")
# Se visualiza la matriz de correlación de forma gráfica
corrplot(mcorrelacion, type="upper")
# Se visualiza la matriz de correlación de forma gráfica
corrplot(mcorrelacion, type="upper", addCoef.col = "black")
# --- Elaboración de la matriz de correlación ---
# Se usa la función cor la cual calcula la correlación de variables numéricas
# Se usa la función round la cual redondea el resultado a 3 decimales
mcorrelacion <- round(cor(trainCuan),3)
# --- Elaboración de la matriz de correlación ---
# Se usa la función cor la cual calcula la correlación de variables numéricas
# Se usa la función round la cual redondea el resultado a 3 decimales
mcorrelacion <- round(cor(trainCuan),3)
# --- Elaboración de la matriz de correlación ---
# Se usa la función cor la cual calcula la correlación de variables numéricas
# Se usa la función round la cual redondea el resultado a 3 decimales
round(cor(trainCuan),3)
# --- Elaboración de la matriz de correlación ---
# Se usa la función cor la cual calcula la correlación de variables numéricas
# Se usa la función round la cual redondea el resultado a 3 decimales
round(cor(trainCuan),3)
# Se visualiza la matriz de correlación de forma gráfica
corrplot(mcorrelacion, type="upper")
# --- Elaboración de la matriz de correlación ---
# Se usa la función cor la cual calcula la correlación de variables numéricas
# Se usa la función round la cual redondea el resultado a 3 decimales
round(cor(trainCuan,use="complete.obs"),3)
# --- Elaboración de la matriz de correlación ---
# Se usa la función cor la cual calcula la correlación de variables numéricas
# Se usa la función round la cual redondea el resultado a 3 decimales
round(cor(trainCuan,use="complete.obs"),3)
# Se visualiza la matriz de correlación de forma gráfica
corrplot(mcorrelacion, type="upper")
mcorrelacion <- round(cor(trainCuan,use="complete.obs"),3)
# Se visualiza la matriz de correlación de forma gráfica
corrplot(mcorrelacion, type="upper")
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
install.packages("ggpubr")
corrplot(mcorrelacion, type="upper")
corrplot(mcorrelacion, type="lower")
?require
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
View(mcorrelacion)
# Se visualiza la matriz de correlación de forma gráfica
corrplot(mcorrelacion, type="upper")
corrplot(mcorrelacion, type="lower")
install.packages(c("e1071", "factoextra", "fpc", "mclust", "NbClust"))
ss <- (nrow(iris[,1:4])-1)*sum(apply(iris[,1:4],2,var))
for (i in 2:10)
wss[i] <- sum(kmeans(iris[,1:4], centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
wss <- (nrow(iris[,1:4])-1)*sum(apply(iris[,1:4],2,var))
for (i in 2:10)
wss[i] <- sum(kmeans(iris[,1:4], centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
?ggpubr
?ggpubr
?corrplot
?tidyverse
?ggpubr_args
?ggplot2
?tidyverse
iris
summary(iris)
nrow(iris)
?withinss
ncol(iris)
ncol(trainCuan)
?apply
nrow(iris)
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
# ---------- Clustering -----------
wardcluster <- (nrow(trainCuan[,1:35])-1)*sum(apply(trainCuan[,1:35],2,var))
nrow(trainCuan)
wardcluster <- (nrow(trainCuan[,1:35])-1)*sum(apply(trainCuan[,1:35],2,var))
ncol(iris)
plotcluster(iris[,1:4],km$cluster) #grafica la ubicación de los clusters
?apply(array, margin, ...)
iris[,1:4]
ncol(iris)
summary(iris)
nrow(iris[,1:4])
nrow(iris)
(nrow(iris[,1:4])-1)
ncol(trainCuan)
(nrow(trainCuan[,1:35])-1)
(nrow(trainCuan[,1:35]))
nrow(trainCuan[,1:35])
iris[,1:4]
apply(iris[,1:4],2,var)
apply(trainCuan[,1:35],2,var)
sum(apply(trainCuan[,1:35],2,var))
wss <- (nrow(iris[,1:4])-1)*sum(apply(iris[,1:4],2,var))
ward <- (nrow(trainCuan[,1:35])-1)*sum(apply(trainCuan[,1:35],2,var))
trainCuan <- na.omit(trainCuan)
ward <- (nrow(trainCuan[,1:35])-1)*sum(apply(trainCuan[,1:35],2,var))
source('~/Downloads/Ejemplo de Clustering.R', echo=TRUE)
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el n?mero de clusters ?ptimo
library(factoextra) #Para hacer gr?ficos bonitos de clustering
#k-medias
datos<-iris
irisCompleto<-iris[complete.cases(iris),]
km<-kmeans(iris[,1:4],3)
datos$grupo<-km$cluster
g1<- datos[datos$grupo==1,]
prop.table(table(g1$Species))*100
nrow(g1)
summary(g1)
g2<- datos[datos$grupo==2,]
prop.table(table(g2$Species))*100
g3<- datos[datos$grupo==3,]
prop.table(table(g3$Species))*100
plotcluster(iris[,1:4],km$cluster) #grafica la ubicación de los clusters
#Clustering jerárquico
hc<-hclust(dist(iris[,1:4])) #Genera el clustering jerárquico de los datos
plot(hc) #Genera el dendograma
rect.hclust(hc,k=3) #Dibuja el corte de los grupos en el gráfico
groups<-cutree(hc,k=3) #corta el dendograma, determinando el grupo de cada fila
datos$gruposHC<-groups
g1HC<-datos[datos$gruposHC==1,]
g2HC<-datos[datos$gruposHC==2,]
g3HC<-datos[datos$gruposHC==3,]
#Fuzzy C-Means
fcm<-cmeans(iris[,1:4],3)
datos$FCGrupos<-fcm$cluster
datos<-cbind(datos,fcm$membership)
#Mixture of gaussians
mc<-Mclust(iris[,1:4],3)
plot(mc, what = "classification", main="MClust Classification")
datos$mxGau<-mc$classification
g1MC<-datos[datos$mxGau==1,]
g2MC<-datos[datos$mxGau==2,]
g3MC<-datos[datos$mxGau==3,]
#Método de la silueta para las k-medias
silkm<-silhouette(km$cluster,dist(iris[,1:4]))
mean(silkm[,3]) #0.55, no es la mejor partición pero no está mal
#Método de la silueta para clustering jerárquico
silch<-silhouette(groups,dist(iris[,1:4]))
mean(silch[,3]) #0.51, no es la mejor partición pero no está mal
#Método de la silueta para fuzzy cmeans
silfcm<-silhouette(fcm$cluster,dist(iris[,1:4]))
mean(silfcm[,3]) #0.54, no es la mejor partición pero no está mal
#Método de la silueta para mixture of gaussians
silmg<-silhouette(mc$classification,dist(iris[,1:4]))
mean(silmg[,3]) #0.50, no es la mejor partición pero no está mal
#Método de Ward para determinar el número correcto de clusteres con k-medias
#Para saber cual es el mejor numero de clusters
wss <- (nrow(iris[,1:4])-1)*sum(apply(iris[,1:4],2,var))
for (i in 2:10)
wss[i] <- sum(kmeans(iris[,1:4], centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
#Paquete para saber el mejor n?mero de clusters
nb <- NbClust(iris[,1:4], distance = "euclidean", min.nc = 2,
max.nc = 10, method = "complete", index ="all")
#Visualizaci?n de los clusters con factoextra
#Visualizaci?n de las k-medias
fviz_cluster(km, data = iris[,1:4],geom = "point", ellipse.type = "norm")
#Visualizaci?n de cluster jer?rquico
hc.cut<-hcut(iris[,1:4], k=3, hc_method = "complete")
fviz_dend(hc.cut, show_labels = FALSE, rect = TRUE)
fviz_cluster(hc.cut, ellipse.type = "convex")
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R', echo=TRUE)
plot(1:10, ward, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
# Se calcula la cantidad de clusters necesarios según el método de Ward
ward <- (nrow(trainCuan[,1:35])-1)*sum(apply(trainCuan[,1:35],2,var))
for (i in 2:20)
ward[i] <- sum(kmeans(trainCuan[,1:35], centers=i)$withinss)
# Se grafica el resultado para visualizar mejor
plot(1:20, ward, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
# Se calcula la cantidad de clusters necesarios según el método de Ward
ward <- (nrow(trainCuan[,1:35])-1)*sum(apply(trainCuan[,1:35],2,var))
for (i in 1:10)
ward[i] <- sum(kmeans(trainCuan[,1:35], centers=i)$withinss)
# Se grafica el resultado para visualizar mejor
plot(1:10, ward, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
# ---------- Clustering -----------
# Elimina los NAs de las variables cuantitativas
trainCuan <- na.omit(trainCuan)
# Se calcula la cantidad de clusters necesarios según el método de Ward
ward <- (nrow(trainCuan[,1:35])-1)*sum(apply(trainCuan[,1:35],2,var))
for (i in 2:10)
ward[i] <- sum(kmeans(trainCuan[,1:35], centers=i)$withinss)
# Se grafica el resultado para visualizar mejor
plot(1:10, ward, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
?kmeans
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:35],5)
numericas$grupo <- km$cluster
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:35],5)
numericas$grupo <- km$cluster
plotcluster(trainCuan[,1:35],km$cluster)
plotcluster(trainCuan[,0:35],km$cluster)
summary(trainCuan)
ward <- (nrow(trainCuan[,1:36])-1)*sum(apply(trainCuan[,1:36],2,var))
for (i in 2:10)
ward[i] <- sum(kmeans(trainCuan[,1:36], centers=i)$withinss)
# Se grafica el resultado para visualizar mejor
plot(1:10, ward, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
# K-medias
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:36],5)
numericas$grupo <- km$cluster
plotcluster(trainCuan[,1:36],km$cluster)
km <- kmeans(trainCuan[,1:36],6)
numericas$grupo <- km$cluster
plotcluster(trainCuan[,1:36],km$cluster)
plotcluster(trainCuan[,1:36],km$cluster)
km <- kmeans(trainCuan[,1:36],5)
numericas$grupo <- km$cluster
plotcluster(trainCuan[,1:36],km$cluster)
silcluster <- silhouette(km$cluster,dist(trainCuan[,1:36]))
mean(silcluster[,5])
silkm<-silhouette(km$cluster,dist(iris[,1:4]))
mean(silkm[,3])
mean(silkm[,3])
mean(silcluster[,5])
mean(silcluster[,4])
fviz_cluster(km, data = trainCuan[,1:36],geom = "point", ellipse.type = "norm")
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:36],7)
numericas$grupo <- km$cluster
plotcluster(trainCuan[,1:36],km$cluster)
fviz_cluster(km, data = trainCuan[,1:36],geom = "point", ellipse.type = "norm")
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:36],4)
numericas$grupo <- km$cluster
fviz_cluster(km, data = trainCuan[,1:36],geom = "point", ellipse.type = "norm")
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:36],3)
numericas$grupo <- km$cluster
fviz_cluster(km, data = trainCuan[,1:36],geom = "point", ellipse.type = "norm")
plot(1:10, ward, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R')
mean(silcluster[,5])
?silhouette
View(km)
library(cluster) #Para calcular la silueta
require(cluster) #Para calcular la silueta
silcluster <- silhouette(km$cluster,dist(trainCuan[,1:36]))
mean(silcluster[,5])
# Método de la silueta
silcluster <- silhouette(km$cluster,dist(trainCuan[,1:36]))
mean(silcluster)
mean(silkm)
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el n?mero de clusters ?ptimo
library(factoextra) #Para hacer gr?ficos bonitos de clustering
#k-medias
datos<-iris
irisCompleto<-iris[complete.cases(iris),]
km<-kmeans(iris[,1:4],3)
datos$grupo<-km$cluster
g1<- datos[datos$grupo==1,]
prop.table(table(g1$Species))*100
nrow(g1)
summary(g1)
g2<- datos[datos$grupo==2,]
prop.table(table(g2$Species))*100
g3<- datos[datos$grupo==3,]
prop.table(table(g3$Species))*100
plotcluster(iris[,1:4],km$cluster) #grafica la ubicación de los clusters
#Clustering jerárquico
hc<-hclust(dist(iris[,1:4])) #Genera el clustering jerárquico de los datos
plot(hc) #Genera el dendograma
rect.hclust(hc,k=3) #Dibuja el corte de los grupos en el gráfico
groups<-cutree(hc,k=3) #corta el dendograma, determinando el grupo de cada fila
datos$gruposHC<-groups
g1HC<-datos[datos$gruposHC==1,]
g2HC<-datos[datos$gruposHC==2,]
g3HC<-datos[datos$gruposHC==3,]
#Fuzzy C-Means
fcm<-cmeans(iris[,1:4],3)
datos$FCGrupos<-fcm$cluster
datos<-cbind(datos,fcm$membership)
#Mixture of gaussians
mc<-Mclust(iris[,1:4],3)
plot(mc, what = "classification", main="MClust Classification")
datos$mxGau<-mc$classification
g1MC<-datos[datos$mxGau==1,]
g2MC<-datos[datos$mxGau==2,]
g3MC<-datos[datos$mxGau==3,]
#Método de la silueta para las k-medias
silkm<-silhouette(km$cluster,dist(iris[,1:4]))
mean(silkm[,3]) #0.55, no es la mejor partición pero no está mal
#Método de la silueta para clustering jerárquico
silch<-silhouette(groups,dist(iris[,1:4]))
mean(silch[,3]) #0.51, no es la mejor partición pero no está mal
#Método de la silueta para fuzzy cmeans
silfcm<-silhouette(fcm$cluster,dist(iris[,1:4]))
mean(silfcm[,3]) #0.54, no es la mejor partición pero no está mal
#Método de la silueta para mixture of gaussians
silmg<-silhouette(mc$classification,dist(iris[,1:4]))
mean(silmg[,3]) #0.50, no es la mejor partición pero no está mal
#Método de Ward para determinar el número correcto de clusteres con k-medias
#Para saber cual es el mejor numero de clusters
wss <- (nrow(iris[,1:4])-1)*sum(apply(iris[,1:4],2,var))
for (i in 2:10)
wss[i] <- sum(kmeans(iris[,1:4], centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
#Paquete para saber el mejor n?mero de clusters
nb <- NbClust(iris[,1:4], distance = "euclidean", min.nc = 2,
max.nc = 10, method = "complete", index ="all")
#Visualizaci?n de los clusters con factoextra
#Visualizaci?n de las k-medias
fviz_cluster(km, data = iris[,1:4],geom = "point", ellipse.type = "norm")
#Visualizaci?n de cluster jer?rquico
hc.cut<-hcut(iris[,1:4], k=3, hc_method = "complete")
fviz_dend(hc.cut, show_labels = FALSE, rect = TRUE)
fviz_cluster(hc.cut, ellipse.type = "convex")
mean(silkm)
mean(silkm[,3])
mean(silcluster[,3])
nb <- NbClust(iris[,1:4], distance = "euclidean", min.nc = 2,
max.nc = 10, method = "complete", index ="all")
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R')
mean(silcluster[,3])
View(datos)
View(datos)
View(km)
View(mcorrelacion)
View(mcorrelacion)
View(numericas)
View(numericas)
View(todoNumericas)
View(train)
trainCuan[,1:36]
trainCuan[,1:35]
View(train)
fviz_nbclust(numericas, km, method = "silhouette", k.max = 24) + theme_minimal() + ggtitle("The Silhouette Plot")
library(NbClust) #Para determinar el numero de clusters optimo
fviz_nbclust(nume, kmeans, method = "silhouette", k.max = 24) + theme_minimal() + ggtitle("The Silhouette Plot")
nume <- scale(numericas)
fviz_nbclust(nume, kmeans, method = "silhouette", k.max = 24) + theme_minimal() + ggtitle("The Silhouette Plot")
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:36],3)
numericas$grupo <- km$cluster
viz_cluster(km, data = trainCuan[,1:36],geom = "point", ellipse.type = "norm")
fviz_cluster(km, data = trainCuan[,1:36],geom = "point", ellipse.type = "norm")
silcluster <- silhouette(km$cluster,dist(trainCuan[,1:36]))
mean(silcluster[,3])
fviz_nbclust(nume, kmeans, method = "silhouette", k.max = 20) + theme_minimal() + ggtitle("The Silhouette Plot")
fviz_nbclust(nume, kmeans, method = "silhouette", k.max = 10) + theme_minimal() + ggtitle("The Silhouette Plot")
plotcluster(trainCuan[,1:36],km$cluster)
?scale
fviz_nbclust(numericas, kmeans, method = "silhouette", k.max = 10) + theme_minimal() + ggtitle("The Silhouette Plot")
mean(silcluster[,3])
mean(silcluster[,2])
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:36],2)
numericas$grupo <- km$cluster
silcluster <- silhouette(km$cluster,dist(trainCuan[,1:36]))
mean(silcluster[,2])
mean(silcluster[,2])
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:36],2)
numericas$grupo <- km$cluster
silcluster <- silhouette(km$cluster,dist(trainCuan[,1:36]))
mean(silcluster[,2])
numericas <- trainCuan
todoNumericas <- trainCuan[complete.cases(trainCuan),]
km <- kmeans(trainCuan[,1:36],3)
numericas$grupo <- km$cluster
silcluster <- silhouette(km$cluster,dist(trainCuan[,1:36]))
mean(silcluster[,3])
fviz_nbclust(numericas, kmeans, method = "silhouette", k.max = 10) + theme_minimal() + ggtitle("The Silhouette Plot")
fviz_nbclust(scale(numericas), kmeans, method = "silhouette", k.max = 10) + theme_minimal() + ggtitle("The Silhouette Plot")
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R')
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R')
plot(1:10, ward, type="b", xlab="Numero de clusters",  ylab="Within groups sum of squares")
mean(silcluster[,3])
numericas$grupo
#
fviz_cluster(km, data = trainCuan[,1:36],geom = "point", ellipse.type = "norm")
mean(silcluster[,3])
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R')
# Se grafica el resultado para visualizar mejor
plot(1:10, ward, type="b", xlab="Numero de clusters",  ylab="Grupos suma de cuadrados")
# Se grafican los 3 grupos de cluster
fviz_cluster(km, data = trainCuan[,1:36],geom = "point", ellipse.type = "norm")
# Se grafican los 3 grupos de cluster
fviz_cluster(km, data = trainCuan[,1:36],geom = "point", ellipse.type = "norm")
# Grafica el numero optimo de clusters
fviz_nbclust(scale(numericas), kmeans, method = "silhouette", k.max = 10) + theme_minimal() + ggtitle("The Silhouette Plot")
View(mcorrelacion)
source('~/Documents/Data Science/Lab1DataScience/Laboratorio1.R')
KMOS()
KMO(corre) #Mala adecuacion muestral
bartlett.test(trainCuan) # valor p aproximadamente 0, por lo tanto hay suficiente info para rechazar Ho
pcaTrainCuan <- PCA(trainCuan)
summary(pcaTrainCuan)
bart_spher(mcorrelacion)
install.packages("rela")
install.packages("psych")
install.packages("FactoMineR")
install.packages("corrplot")
install.packages("cluster")
install.packages("fpc")
install.packages("NbClust")
install.packages("factoextra")
install.packages("REdaS")
KMO(corre) #Mala adecuacion muestral
bartlett.test(trainCuan) # valor p aproximadamente 0, por lo tanto hay suficiente info para rechazar Ho
pcaTrainCuan <- PCA(trainCuan)
summary(pcaTrainCuan)
bart_spher(mcorrelacion)
library(REdaS)
bart_spher(mcorrelacion)
summary(pcaTrainCuan)
# ---------- PCA -----------
KMO(corre) #Mala adecuacion muestral
library(psych) #Para poder utilizar KMO()
# ---------- PCA -----------
KMO(corre) #Mala adecuacion muestral
corre <- cor(trainCuan)
# ---------- PCA -----------
KMO(corre) #Mala adecuacion muestral
bartlett.test(trainCuan) # valor p aproximadamente 0, por lo tanto hay suficiente info para rechazar Ho
bart_spher(mcorrelacion)
bartlett.test(trainCuan) # valor p aproximadamente 0, por lo tanto hay suficiente info para rechazar Ho
pcaTrainCuan <- PCA(trainCuan)
library(FactoMineR)
pcaTrainCuan <- PCA(trainCuan)
summary(pcaTrainCuan)
install.packages("arules")
library(arules) # Reglas de asociacion
# ------------- Apriori --------------
rulesAP <- apriori(trainCuan, parameter = list(support = 0.2,
confidence = 0.70,
target = "rules"))
rulesAP <- apriori(trainCuan, parameter = list(support = 0.2,
confidence = 0.70,
target = "rules"))
# ------------- Apriori --------------
rulesAP <- apriori(mcorrelacion, parameter = list(support = 0.2,
confidence = 0.70,
target = "rules"))
stop
stop()
!q
summary(pcaTrainCuan)
# ------------- Apriori --------------
rulesAP <- apriori(data, parameter = list(support = 0.5, condifence = 0.8, maxlen = 10, maxtime=5, target = "rules"))
rulesAP <- apriori(data, parameter = list(support = 0.5, condifence = 0.8, maxlen = 10, maxtime=5, target = "rules"))
# ------------- Apriori --------------
rulesAP <- apriori(trainCuan, parameter = list(support = 0.5, condifence = 0.8, maxlen = 10, maxtime=5, target = "rules"))
trainCuan <- as.factor(trainCuan)
rulesAP <- apriori(trainCuan, parameter = list(support = 0.5, condifence = 0.8, maxlen = 10, maxtime=5, target = "rules"))
# ------------- Apriori --------------
rulesAP <- apriori(mcorrelacion, parameter = list(support = 0.5, condifence = 0.8, maxlen = 10, maxtime=5, target = "rules"))
# ------------- Apriori --------------
trainCuan
View(train)
summary(train)
typeof(train)
typeof(trainCuan)
# ------------- Apriori --------------
trainCuan <- as.data.frame(trainCuan)
View(trainCuan)
